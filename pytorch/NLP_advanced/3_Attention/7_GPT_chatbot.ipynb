{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf71f61b-bfd9-4aba-ad83-941ea0c6629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a98d6c-81c4-4e5c-a6be-44ba5b15271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amps\\anaconda3\\envs\\dl_study\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\amps\\anaconda3\\envs\\dl_study\\lib\\site-packages\\transformers\\modeling_utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc5e40e-af00-4251-858d-0bb35a8eb54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "3\n",
      "----------\n",
      "</s>\n",
      "<usr>\n",
      "<pad>\n",
      "<sys>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print('-' * 10)\n",
    "print(tokenizer.decode(1))\n",
    "print(tokenizer.decode(2))\n",
    "print(tokenizer.decode(3))\n",
    "print(tokenizer.decode(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044c509-896b-459c-8194-f19e887a9044",
   "metadata": {},
   "source": [
    "# 챗봇 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325d9451-9e46-44f9-becf-6ffd4420d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a065d51-79cf-47cf-87e0-364da6e75a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9eea360-87b4-4947-b782-f561215a87e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 샘플의 개수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('챗봇 샘플의 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556d37c-0ea5-4d34-8a18-95b7efb09f93",
   "metadata": {},
   "source": [
    "# 챗봇 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18acc6ca-37aa-410d-97b8-016e08f6d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # 한 번에 처리할 데이터 샘플의 수를 지정\n",
    "\n",
    "import tqdm  # 학습 진행 상황을 시각적으로 보여주는 모듈\n",
    "import torch  # PyTorch 라이브러리\n",
    "from torch.utils.data import Dataset, DataLoader  # 데이터셋과 데이터로더를 다루는 모듈\n",
    "\n",
    "# 대화 데이터를 위한 사용자 정의 데이터셋 클래스 정의\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, train_data, tokenizer):\n",
    "        self.train_data = train_data  # 학습 데이터를 저장\n",
    "        self.tokenizer = tokenizer  # 텍스트를 토큰으로 변환할 토크나이저 저장\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)  # 데이터셋의 크기(샘플 수)를 반환\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.train_data.Q.iloc[idx]  # 인덱스에 해당하는 질문 텍스트 가져오기\n",
    "        answer = self.train_data.A.iloc[idx]  # 인덱스에 해당하는 답변 텍스트 가져오기\n",
    "        bos_token = self.tokenizer.bos_token_id  # 문장의 시작을 나타내는 토큰 ID\n",
    "        eos_token = self.tokenizer.eos_token_id  # 문장의 끝을 나타내는 토큰 ID\n",
    "        # 질문과 답변을 하나의 문자열로 연결하여 토큰화\n",
    "        sent = self.tokenizer.encode('' + question + '' + answer, add_special_tokens=False)\n",
    "        # 시작과 끝 토큰을 포함한 텐서를 반환\n",
    "        return torch.tensor([bos_token] + sent + [eos_token], dtype=torch.long)\n",
    "\n",
    "# 배치의 시퀀스를 패딩하여 같은 길이로 맞추는 함수 정의\n",
    "def collate_fn(batch):\n",
    "    return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "batch_size = 32  # 배치 크기를 다시 설정\n",
    "chat_dataset = ChatDataset(train_data, tokenizer)  # 데이터셋 인스턴스 생성\n",
    "data_loader = DataLoader(chat_dataset, batch_size=batch_size, collate_fn=collate_fn)  # 데이터로더 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaab5f1-df3b-48a9-b0cd-c975c149f1b4",
   "metadata": {},
   "source": [
    "# 챗봇 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ad5525-53e8-488c-b253-c06b1e65b1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, eps=1e-08)\n",
    "\n",
    "steps = len(train_data) // batch_size + 1\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e426ac-5fa5-4929-9a0f-09a16587e14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "# GPU가 사용 가능한지 확인하고, 가능하면 \"cuda\"를 선택, 그렇지 않으면 CPU를 선택\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 선택된 디바이스(GPU 또는 CPU)로 이동\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c66c70-b4a8-4d3d-8af9-7e3a5a2e1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 370/370 [00:48<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 2.31870702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 370/370 [00:47<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    2] cost = 1.83392793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 370/370 [00:48<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    3] cost = 1.45199434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 주어진 에포크 수만큼 학습 루프를 반복\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0  # 에포크 손실 초기화\n",
    "\n",
    "    # 데이터 로더에서 배치를 하나씩 가져와서 학습을 진행\n",
    "    for batch in tqdm.tqdm(data_loader, total=steps):\n",
    "        # 배치를 선택한 디바이스로 이동\n",
    "        batch = batch.to(device)\n",
    "        # 레이블을 배치와 동일하게 설정 (입력을 그대로 레이블로 사용)\n",
    "        labels = batch.clone()\n",
    "        # 옵티마이저의 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 모델에 입력을 주고, 출력과 손실값을 계산\n",
    "        result = model(input_ids=batch, labels=labels)\n",
    "        loss = result.loss  # 계산된 손실값\n",
    "        batch_loss = loss.mean()  # 배치 손실 계산\n",
    "\n",
    "        # 손실값에 대해 역전파를 통해 기울기 계산\n",
    "        batch_loss.backward()\n",
    "        # 옵티마이저를 통해 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        # 에포크 손실에 이번 배치의 손실값을 추가\n",
    "        epoch_loss += batch_loss.item() / steps\n",
    "\n",
    "    # 현재 에포크가 끝난 후 평균 손실값 출력\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e4153-c46d-4d17-9c44-306c5327393c",
   "metadata": {},
   "source": [
    "# 챗봇 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2e6d08-b389-40a6-b76a-aaf87627696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><usr> 오늘도 좋은 하루!<sys><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '<usr>'는 사용자 입력을 '<sys>'는 시스템 응답을 나타내는 태그로 감싸서 대화 형태로 변환\n",
    "text = '오늘도 좋은 하루!'\n",
    "sent = '<usr>' + text + '<sys>'\n",
    "\n",
    "# 문장의 시작을 알리는 bos_token_id와 토큰화 된 문장을 이어 붙이고 정수 인코딩.\n",
    "# 즉, </s>를 맨 앞에 부착 후 정수 인코딩.\n",
    "input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "# 모델을 사용해 주어진 입력에 대한 응답을 생성 (최대 50개의 토큰, 조기 종료 조건 설정)\n",
    "output = model.generate(input_ids, max_length=50, early_stopping=True, eos_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c97eb87-f54c-4ffe-adc4-c3780af10e33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoded_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecoded_sentence\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sys> \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoded_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "print(decoded_sentence.split('<sys> ')[1].replace('</s>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "199a101c-1282-4785-8fe1-9873c001526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer_by_chatbot(user_text):\n",
    "    sent = '' + user_text + ''\n",
    "    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent, add_special_tokens=False)\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    output = model.generate(input_ids, max_length=50, do_sample=True, top_k=2)\n",
    "    sentence = tokenizer.decode(output[0].tolist())\n",
    "    chatbot_response = sentence.split(' ')[1].replace('', '')\n",
    "    return chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97da30d3-c785-4792-82cd-a7982be1caa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer_by_chatbot('안녕! 반가워~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb4d4aa5-a84a-4325-8608-4dbd46da2fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너는'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer_by_chatbot('너는 누구야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4330edf1-c8e1-472a-b74a-5c2f8f4fe037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너무'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer_by_chatbot('너무 심심한데 나랑 놀자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918c171c-ab82-45a2-a814-fb2577313597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'영화'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer_by_chatbot('영화 해리포터 재밌어?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "939a21a4-d1dd-4c3b-8298-5aa08036b4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer_by_chatbot('너 딥 러닝 잘해?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20372-acb9-4bb3-a935-fa0f4be8e876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_study",
   "language": "python",
   "name": "dl_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
