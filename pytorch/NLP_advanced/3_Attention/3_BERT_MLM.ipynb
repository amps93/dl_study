{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088691ec-bef8-49c5-af00-df114ff84bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3fab91-c1a3-404a-ac3d-dc289ea3ecdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3991932e648400cac6872505cb5cf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amps\\anaconda3\\envs\\dl_study\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amps\\.cache\\huggingface\\hub\\models--bert-large-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd13d2ce781647a5981f376d5ad8c13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f78c345b6e145ccb91c433276d5eed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5456944a5f2e472bb4676fed208646a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6660fbe3b2c94554bc57e7f85b68edf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "BertForMaskedLM.from_pretrained('BERT ëª¨ë¸ ì´ë¦„')ì„ ë„£ìœ¼ë©´ [MASK]ë¼ê³  ë˜ì–´ìžˆëŠ” ë‹¨ì–´ë¥¼ ë§žì¶”ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ êµ¬ì¡°ë¡œ BERTë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "ë‹¤ì‹œ ë§í•´ì„œ BERTë¥¼ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ í˜•íƒœë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "AutoTokenizer.from_pretrained('ëª¨ë¸ ì´ë¦„')ì„ ë„£ìœ¼ë©´ í•´ë‹¹ ëª¨ë¸ì´ í•™ìŠµë˜ì—ˆì„ ë‹¹ì‹œì— ì‚¬ìš©ë˜ì—ˆë˜ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831bbd2b-1f0e-4023-b2ea-e9559e94e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('Soccer is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c331b4-2a6c-4f87-94d1-a68144c3ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4715, 2003, 1037, 2428, 4569, 103, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ë¥¼ í™•ì¸\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c21fd9-af78-40d2-b4f1-25c912ab8264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ìž¥ì„ êµ¬ë¶„í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”© ê²°ê³¼ë¥¼ í™•ì¸\n",
    "print(inputs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d20846-5514-43dd-8eff-9c349983e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤ì œ ë‹¨ì–´ì™€ íŒ¨ë”© í† í°ì„ êµ¬ë¶„í•˜ëŠ” ìš©ë„ì¸ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í™•ì¸\n",
    "# í˜„ìž¬ì˜ ìž…ë ¥ì—ì„œëŠ” íŒ¨ë”©ì´ ì—†ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¬¸ìž¥ ê¸¸ì´ë§Œí¼ì˜ 1 ì‹œí€€ìŠ¤ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1aef9-f1a5-4943-9093-4057e71faa80",
   "metadata": {},
   "source": [
    "# mask í† í° ì˜ˆì¸¡í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7aaabb7-a32b-46cf-941e-447cec131e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "\n",
    "# FillMaskPipelineì€ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì •í•˜ë©´ ì†ì‰½ê²Œ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì •ë¦¬í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed54431d-83c0-4265-be13-f3fbe7a3ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amps\\anaconda3\\envs\\dl_study\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7621217966079712,\n",
       "  'token': 4368,\n",
       "  'token_str': 'sport',\n",
       "  'sequence': 'soccer is a really fun sport.'},\n",
       " {'score': 0.20342297852039337,\n",
       "  'token': 2208,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'soccer is a really fun game.'},\n",
       " {'score': 0.012208719737827778,\n",
       "  'token': 2518,\n",
       "  'token_str': 'thing',\n",
       "  'sequence': 'soccer is a really fun thing.'},\n",
       " {'score': 0.0018630522536113858,\n",
       "  'token': 4023,\n",
       "  'token_str': 'activity',\n",
       "  'sequence': 'soccer is a really fun activity.'},\n",
       " {'score': 0.0013355060946196318,\n",
       "  'token': 2492,\n",
       "  'token_str': 'field',\n",
       "  'sequence': 'soccer is a really fun field.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìž…ë ¥ ë¬¸ìž¥ìœ¼ë¡œë¶€í„° [MASK]ì˜ ìœ„ì¹˜ì— ë“¤ì–´ê°ˆ ìˆ˜ ìžˆëŠ” ìƒìœ„ 5ê°œì˜ í›„ë³´ ë‹¨ì–´ë“¤ì„ ì¶œë ¥\n",
    "pip('Soccer is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a284f8-b68d-4b4e-aea1-8552e1aa6968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2562906742095947,\n",
       "  'token': 2265,\n",
       "  'token_str': 'show',\n",
       "  'sequence': 'the avengers is a really fun show.'},\n",
       " {'score': 0.17284217476844788,\n",
       "  'token': 3185,\n",
       "  'token_str': 'movie',\n",
       "  'sequence': 'the avengers is a really fun movie.'},\n",
       " {'score': 0.11107778549194336,\n",
       "  'token': 2466,\n",
       "  'token_str': 'story',\n",
       "  'sequence': 'the avengers is a really fun story.'},\n",
       " {'score': 0.07249010354280472,\n",
       "  'token': 2186,\n",
       "  'token_str': 'series',\n",
       "  'sequence': 'the avengers is a really fun series.'},\n",
       " {'score': 0.07046669721603394,\n",
       "  'token': 2143,\n",
       "  'token_str': 'film',\n",
       "  'sequence': 'the avengers is a really fun film.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('The Avengers is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdba3538-0e9a-493a-a289-ff5f7e7863b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3573077321052551,\n",
       "  'token': 2147,\n",
       "  'token_str': 'work',\n",
       "  'sequence': 'i went to work this morning.'},\n",
       " {'score': 0.23304490745067596,\n",
       "  'token': 2793,\n",
       "  'token_str': 'bed',\n",
       "  'sequence': 'i went to bed this morning.'},\n",
       " {'score': 0.12845094501972198,\n",
       "  'token': 2082,\n",
       "  'token_str': 'school',\n",
       "  'sequence': 'i went to school this morning.'},\n",
       " {'score': 0.06230591982603073,\n",
       "  'token': 3637,\n",
       "  'token_str': 'sleep',\n",
       "  'sequence': 'i went to sleep this morning.'},\n",
       " {'score': 0.04695272445678711,\n",
       "  'token': 2465,\n",
       "  'token_str': 'class',\n",
       "  'sequence': 'i went to class this morning.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('I went to [MASK] this morning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7cb18-4a04-4ff4-88f9-790e8e24aecd",
   "metadata": {},
   "source": [
    "# 4. ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f9d7a99-3624-4022-939a-d39313c8655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# Base Model (108M)\n",
    "model = BertForMaskedLM.from_pretrained('klue/bert-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25a2dbfa-6392-4899-97bd-d61ad3f23683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4713, 2259, 3944, 6001, 2259, 4, 809, 18, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” [MASK]ë‹¤.')\n",
    "print(inputs['input_ids'])\n",
    "print(inputs['token_type_ids'])\n",
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1df28170-e565-42fe-b508-a7ce51880c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "379b33c6-e352-4ae0-8185-f91b31537194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8963631987571716,\n",
       "  'token': 4559,\n",
       "  'token_str': 'ìŠ¤í¬ì¸ ',\n",
       "  'sequence': 'ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” ìŠ¤í¬ì¸  ë‹¤.'},\n",
       " {'score': 0.0259579885751009,\n",
       "  'token': 568,\n",
       "  'token_str': 'ê±°',\n",
       "  'sequence': 'ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” ê±° ë‹¤.'},\n",
       " {'score': 0.010034081526100636,\n",
       "  'token': 3682,\n",
       "  'token_str': 'ê²½ê¸°',\n",
       "  'sequence': 'ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” ê²½ê¸° ë‹¤.'},\n",
       " {'score': 0.007924444042146206,\n",
       "  'token': 4713,\n",
       "  'token_str': 'ì¶•êµ¬',\n",
       "  'sequence': 'ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” ì¶•êµ¬ ë‹¤.'},\n",
       " {'score': 0.007844283245503902,\n",
       "  'token': 5845,\n",
       "  'token_str': 'ë†€ì´',\n",
       "  'sequence': 'ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” ë†€ì´ ë‹¤.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('ì¶•êµ¬ëŠ” ì •ë§ ìž¬ë¯¸ìžˆëŠ” [MASK]ë‹¤.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a768fa-d56d-4e1e-9e64-f65ae0024463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_study",
   "language": "python",
   "name": "dl_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
