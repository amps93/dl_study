{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b6e4c9-1c3d-4c65-9330-2e99314cf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForNextSentencePrediction\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08045685-59f0-4070-ad35-d5dac079dce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc8ace6e69446e5a34965071621ac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amps\\anaconda3\\envs\\dl_study\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amps\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bac3133-b12b-42a7-8dc6-f3363b44794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78386c8f-52bd-49c9-ba2c-e5099bc47a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c351dd5f-2c10-46ca-a299-bae7974a21fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1999,  3304,  1010, 10733,  2366,  1999,  5337, 10906,  1010,\n",
      "          2107,  2004,  2012,  1037,  4825,  1010,  2003,  3591,  4895, 14540,\n",
      "          6610,  2094,  1012,   102, 10733,  2003,  8828,  2007,  1996,  2224,\n",
      "          1997,  1037,  5442,  1998,  9292,  1012,  1999, 10017, 10906,  1010,\n",
      "          2174,  1010,  2009,  2003,  3013,  2046, 17632,  2015,  2000,  2022,\n",
      "          8828,  2096,  2218,  1999,  1996,  2192,  1012,   102]])\n",
      "[CLS] : 101\n",
      "[SEP] : 102\n"
     ]
    }
   ],
   "source": [
    "print(encoding['input_ids'])\n",
    "print(tokenizer.cls_token, ':', tokenizer.cls_token_id)  # 101 - [CLS]\n",
    "print(tokenizer.sep_token, ':' , tokenizer.sep_token_id)  # 102 - [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d136c1d-c6f8-42c8-b1c7-2aed6021275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] in italy, pizza served in formal settings, such as at a restaurant, is presented unsliced. [SEP] pizza is eaten with the use of a knife and fork. in casual settings, however, it is cut into wedges to be eaten while held in the hand. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 결과 디코딩\n",
    "# BERT에서 두 개의 문장이 입력으로 들어갈 경우에는 맨 앞에는 [CLS] 토큰이 존재하고, \n",
    "# 첫번째 문장이 끝나면 [SEP] 토큰, 그리고 두번째 문장이 종료되었을 때 다시 추가적으로 [SEP] 토큰이 추가됩니다.\n",
    "print(tokenizer.decode(encoding['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a6fb0a-6195-44a3-82d3-d5f1d36f23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 세그먼트 인코딩 결과를 확인\n",
    "# 0이 연속적으로 등장하다가 어느 순간부터 1이 연속적으로 등장하는데, \n",
    "# 이는 [CLS] 토큰의 위치부터 첫번째 문장이 끝나고나서 등장한 [SEP] 토큰까지의 위치에는 0이 등장하고, 다음 두번째 문장부터는 1이 등장하는 것입니다. \n",
    "# token_type_ids에서는 0과 1로 두 개의 문장을 구분하고 있습니다.\n",
    "print(encoding['token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00130a80-0ec5-46e5-8c55-4c12162ba80b",
   "metadata": {},
   "source": [
    "# 다음 문장 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8ee41d-98e4-435a-84b5-c2a827d4270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 2.8382e-06]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  모델에 입력을 넣으면, 해당 모델은 소프트맥스 함수를 지나기 전의 값을 리턴\n",
    "pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])\n",
    "probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기\n",
    "print(probs)  # 모델이 예측한 레이블은 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e02d204-9144-4a0a-874d-ad0c6969a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 예측 레이블 : 0\n"
     ]
    }
   ],
   "source": [
    "# BERT는 사전 학습 당시 이어지는 두 개의 문장의 레이블은 0. 이어지지 않는 두 개의 문장의 경우에는 레이블을 1로 두고서 이진 분류로 학습\n",
    "# 위 데이터는 이어진 문장으로 분류\n",
    "next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기\n",
    "print('최종 예측 레이블 :', next_sentence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f97f6e-aefb-441a-be3a-37d0221eb226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 예측 레이블 : 1\n"
     ]
    }
   ],
   "source": [
    "# 상관없는 두 개의 문장\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])\n",
    "probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기\n",
    "next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기\n",
    "print('최종 예측 레이블 :', next_sentence_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dec794-d30d-453f-b75a-442e6298fbdb",
   "metadata": {},
   "source": [
    "# 4. 한국어 모델의 다음 문장 예측 모델과 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29fab7a1-50de-4b10-961e-e2709b862f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('klue/bert-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a29f1f1f-cec9-4fab-972a-0592759d2572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 예측 레이블 : 0\n"
     ]
    }
   ],
   "source": [
    "# 이어지는 두 개의 문장\n",
    "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
    "next_sentence = \"여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])\n",
    "probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기\n",
    "next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기\n",
    "print('최종 예측 레이블 :', next_sentence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f3dbac3-cf31-44fb-b6e1-9d1228a9e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 예측 레이블 : 1\n"
     ]
    }
   ],
   "source": [
    "# 상관없는 두 개의 문장\n",
    "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
    "next_sentence = \"극장가서 로맨스 영화를 보고싶어요\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "pred = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])\n",
    "probs = torch.nn.functional.softmax(pred.logits, dim=1)  # Softmax 적용하여 확률 얻기\n",
    "next_sentence_label = torch.argmax(probs, dim=1).item()  # 예측된 라벨 얻기\n",
    "print('최종 예측 레이블 :', next_sentence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18000a84-60ab-4e80-8404-4bc0c6238707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_study",
   "language": "python",
   "name": "dl_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
